{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Marinha do Brasil\n",
    "\n",
    "### Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "### Laboratorio de Processamento de Sinais - UFRJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 3.48091125488e-05 seconds\n",
      "Time to read data file: 1.63236904144 seconds\n",
      "Qtd event of 0 is 12939\n",
      "Qtd event of 1 is 29352\n",
      "Qtd event of 2 is 11510\n",
      "Qtd event of 3 is 23760\n",
      "\n",
      "Biggest class is 1 with 29352 events\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from Functions import TrainParameters as trnparams\n",
    "from Functions import TrainFunctions\n",
    "from Functions import FunctionsDataVisualization\n",
    "\n",
    "import multiprocessing \n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print 'Time to import all libraries: '+str(m_time-init_time)+' seconds'\n",
    "\n",
    "analysis_name = 'NeuralNetwork'\n",
    "data_path = os.getenv('OUTPUTDATAPATH')\n",
    "results_path = os.getenv('PACKAGE_NAME')\n",
    "\n",
    "base_results_path = '%s/%s'%(results_path,analysis_name)\n",
    "pict_results_path = '%s/pictures_files'%(base_results_path)\n",
    "files_results_path = '%s/output_files'%(base_results_path)\n",
    "\n",
    "# For multiprocessing purpose\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "database = '4classes'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "spectrum_bins_left = 400\n",
    "development_flag = True\n",
    "development_events = 400\n",
    "\n",
    "if not os.path.exists('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                      (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left)):\n",
    "    print 'No Files in %s/%s\\n'%(data_path,database)\n",
    "else:\n",
    "    #Read lofar data\n",
    "    [data,trgt,class_labels] = joblib.load('%s/%s/lofar_data_file_fft_%i_decimation_%i_spectrum_left_%i.jbl'%\n",
    "                                           (data_path,database,n_pts_fft,decimation_rate,spectrum_bins_left))\n",
    "\n",
    "\n",
    "    m_time = time.time()-m_time\n",
    "    print 'Time to read data file: '+str(m_time)+' seconds'\n",
    "\n",
    "    # correct format\n",
    "    all_data = data\n",
    "    all_trgt = trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))\n",
    "    \n",
    "    # Process data\n",
    "    # unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "    # Same number of events in each class\n",
    "    qtd_events_biggest_class = 0\n",
    "    biggest_class_label = ''\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "            qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "            biggest_class_label = class_label\n",
    "        print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "    print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "    balanced_data = {}\n",
    "    balanced_trgt = {}\n",
    "\n",
    "    from Functions import DataHandler as dh\n",
    "    m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "    for iclass, class_label in enumerate(class_labels):\n",
    "        if development_flag:\n",
    "            class_events = all_data[all_trgt==iclass,:]\n",
    "            if len(balanced_data) == 0:\n",
    "                balanced_data = class_events[0:development_events,:]\n",
    "                balanced_trgt = (iclass)*np.ones(development_events)\n",
    "            else:\n",
    "                balanced_data = np.append(balanced_data,\n",
    "                                          class_events[0:development_events,:], \n",
    "                                          axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,(iclass)*np.ones(development_events))\n",
    "        else:\n",
    "            if len(balanced_data) == 0:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                balanced_data = m_datahandler.CreateEventsForClass(\n",
    "                    class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "                balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "            else:\n",
    "                class_events = all_data[all_trgt==iclass,:]\n",
    "                created_events = (m_datahandler.CreateEventsForClass(all_data[all_trgt==iclass,:],\n",
    "                                                                     qtd_events_biggest_class-\n",
    "                                                                     (len(class_events))))\n",
    "                balanced_data = np.append(balanced_data,created_events,axis=0)\n",
    "                balanced_trgt = np.append(balanced_trgt,\n",
    "                                          (iclass)*np.ones(created_events.shape[0]),axis=0)\n",
    "        \n",
    "    all_data = balanced_data\n",
    "    all_trgt = balanced_trgt\n",
    "\n",
    "    # turn targets in sparse mode\n",
    "    from keras.utils import np_utils\n",
    "    trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of folds must be of Integral type. [ 0.  0.  0. ...,  3.  3.  3.] of type <type 'numpy.ndarray'> was passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7c3ebb12c08c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"# load parameters\\n\\nanalysis_str = 'NeuralNetwork'\\nmodel_prefix_str = 'RawData'\\n\\ntrn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\\nos.remove(trn_params_folder)\\nif not os.path.exists(trn_params_folder):\\n    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=2,\\n                                                         hidden_activation='softplus',\\n                                                         output_activation='softmax',\\n                                                         n_epochs=500,\\n                                                         patience=50,\\n                                                         batch_size=256,\\n                                                         verbose=False)\\n    trn_params.save(trn_params_folder)\\nelse:\\n    trn_params = trnparams.NeuralClassificationTrnParams()\\n    trn_params.load(trn_params_folder)\\n\\nn_folds = 4\\nCVO = trnparams.ClassificationFolds(folder=results_path,\\n                                    n_folds=n_folds,\\n                                    trgt=all_trgt,\\n                                    dev=development_flag, verbose=False)\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/vinicius.mello/.virtualenvs/sonarenvFerney/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/vinicius.mello/.virtualenvs/sonarenvFerney/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinicius.mello/.virtualenvs/sonarenvFerney/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/vinicius.mello/Workspace/SonarAnalysis/Functions/TrainParameters.pyc\u001b[0m in \u001b[0;36mClassificationFolds\u001b[0;34m(folder, n_folds, trgt, dev, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mCVO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mCVO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCVO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCVO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinicius.mello/.virtualenvs/sonarenvFerney/local/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_test_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinicius.mello/.virtualenvs/sonarenvFerney/local/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    275\u001b[0m             raise ValueError('The number of folds must be of Integral type. '\n\u001b[1;32m    276\u001b[0m                              \u001b[0;34m'%s of type %s was passed.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                              % (n_splits, type(n_splits)))\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The number of folds must be of Integral type. [ 0.  0.  0. ...,  3.  3.  3.] of type <type 'numpy.ndarray'> was passed."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load parameters\n",
    "\n",
    "analysis_str = 'NeuralNetwork'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "os.remove(trn_params_folder)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=2,\n",
    "                                                         hidden_activation='softplus',\n",
    "                                                         output_activation='softmax',\n",
    "                                                         n_epochs=500,\n",
    "                                                         patience=50,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "n_folds = 4\n",
    "CVO = trnparams.ClassificationFolds(folder=results_path,\n",
    "                                    n_folds=n_folds,\n",
    "                                    trgt=all_trgt,\n",
    "                                    dev=development_flag, verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = [2,1]\n",
    "hidden_neurons[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento com processamento paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# To train on multiple cores sweeping the number of folds\n",
    "def trainFold(ifold):\n",
    "    ineuron = 20\n",
    "    return TrainFunctions.NeuralTrainFunction(data=all_data,\n",
    "                            trgt=all_trgt,\n",
    "                            ifold=ifold,\n",
    "                            n_folds=n_folds, \n",
    "                            n_neurons=ineuron,\n",
    "                            trn_params=trn_params, \n",
    "                            save_path=results_path,\n",
    "                            dev=development_flag)\n",
    "# To train on multiple cores sweeping the number of neurons\n",
    "def trainNeuron(ineuron):\n",
    "    for ifold in range(len(CVO)):\n",
    "        TrainFunctions.NeuralTrainFunction(data=all_data,\n",
    "                            trgt=all_trgt,\n",
    "                            ifold=ifold,\n",
    "                            n_folds=n_folds, \n",
    "                            n_neurons=ineuron,\n",
    "                            trn_params=trn_params, \n",
    "                            save_path=results_path,\n",
    "                            dev=development_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "p = multiprocessing.Pool(processes=num_processes)\n",
    "start_time = time.time()\n",
    "folds = range(len(CVO))\n",
    "neurons = range(0, 105, 5)\n",
    "# To train on multiple cores sweeping the number of folds\n",
    "#results = p.map(trainFold, folds)\n",
    "\n",
    "# To train on multiple cores sweeping the number of neurons\n",
    "results = p.map(trainNeuron, neurons)\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "end_time = time.time() - start_time\n",
    "print \"It took %.3f seconds to perform the training\"%(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"%.2f minutes\"%(end_time/60)\n",
    "print \"%.2f hours\"%(end_time/3600)\n",
    "print trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análises com variação de Neurônios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neuron variation x MSE\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'NeuralNetwork'\n",
    "model_prefix_str = 'RawData'\n",
    "\n",
    "current_analysis = 'mse'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name, current_analysis)\n",
    "\n",
    "if os.path.exists(analysis_file_name):\n",
    "    os.remove(analysis_file_name)\n",
    "\n",
    "if not os.path.exists(analysis_file_name):\n",
    "    trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "    \n",
    "    if not os.path.exists(trn_params_folder):\n",
    "        trn_params = trnparams.NeuralClassificationTrnParams(n_inits=4,\n",
    "                                                             hidden_activation='softplus',\n",
    "                                                             output_activation='softmax',\n",
    "                                                             n_epochs=500,\n",
    "                                                             patience=50,\n",
    "                                                             batch_size=256,\n",
    "                                                             verbose=False)\n",
    "        trn_params.save(trn_params_folder)\n",
    "    else:\n",
    "        trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "        trn_params.load(trn_params_folder)\n",
    "\n",
    "    params_str = trn_params.get_params_str()\n",
    "    n_folds = 2\n",
    "    CVO = trnparams.ClassificationFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)\n",
    "\n",
    "    neurons_mat = range(0,105,5)\n",
    "\n",
    "    mse_mat = np.zeros([n_folds,len(neurons_mat)])\n",
    "\n",
    "    for ifold in range(len(CVO)):\n",
    "        train_id, test_id = CVO[ifold]\n",
    "\n",
    "        # normalize data based in train set\n",
    "        if trn_params.params['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "        norm_data = scaler.transform(all_data[test_id,:])\n",
    "\n",
    "        for ineuron,neuron_value in enumerate(neurons_mat):     \n",
    "            \n",
    "            model_str = '%s/%s/%s_%i_folds_%s_%i_neurons'%(save_path,analysis_str,\n",
    "                                                       model_prefix_str,\n",
    "                                                       n_folds,\n",
    "                                                       params_str,\n",
    "                                                       neuron_value)\n",
    "            \n",
    "            if not development_flag:        \n",
    "                file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "            else:\n",
    "                file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "            print file_name\n",
    "                \n",
    "            if not os.path.exists(file_name):\n",
    "                def trainFold(ifold):\n",
    "                    ineuron = neuron_value\n",
    "                    return TrainFunctions.NeuralTrainFunction(data=all_data,\n",
    "                                            trgt=all_trgt,\n",
    "                                            ifold=ifold,\n",
    "                                            n_folds=n_folds, \n",
    "                                            n_neurons=ineuron,\n",
    "                                            trn_params=trn_params, \n",
    "                                            save_path=results_path,\n",
    "                                            dev=development_flag)\n",
    "            model = load_model(file_name)\n",
    "            output = model.predict(norm_data)\n",
    "            mse = sklearn.metrics.mean_squared_error(trgt_sparse[test_id,:], np.round(output))\n",
    "            mse_mat[ifold,ineuron] = mse\n",
    "            \n",
    "    joblib.dump([mse_mat,neurons_mat],analysis_file_name,compress=9)\n",
    "else:\n",
    "    [mse_mat,neurons_mat] = joblib.load(analysis_file_name) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10),nrows=1, ncols=1)\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "# Plot Mean Squared Error\n",
    "ax.errorbar(neurons_mat,np.mean(mse_mat,axis=0),\n",
    "            np.std(mse_mat,axis=0),fmt='o-',\n",
    "            color='k',alpha=0.7,linewidth=2.5)\n",
    "\n",
    "ax.set_title('MSE per Hidden Layer size',fontsize=18,weight='bold')\n",
    "ax.set_xlabel('Neurons Quantity',fontsize=18,weight='bold')\n",
    "ax.set_ylabel('MSE Value',fontsize=18,weight='bold') \n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(pict_results_path+'/'+current_analysis+'_'+trn_params.get_params_str()+'.pdf')\n",
    "print \"Topology (%s)\"%trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Neuron variation x Accuracy / Sp index / Efficiency\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'NeuralNetwork'\n",
    "model_prefix_str = 'RawData'\n",
    "current_analysis = 'accuracy_sp_index_effiencyPerClass'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_%s_neuron_number_sweep.jbl'%(results_path,analysis_str,analysis_name,current_analysis)\n",
    "\n",
    "if os.path.exists(analysis_file_name):\n",
    "    os.remove(analysis_file_name)\n",
    "\n",
    "# Check if the analysis has already been done\n",
    "if not os.path.exists(analysis_file_name):\n",
    "    trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "    \n",
    "    # If the model to be analysed has others parameters, delete the trnparams.jbl file and run this cell again\n",
    "    if not os.path.exists(trn_params_folder):\n",
    "        trn_params = trnparams.NeuralClassificationTrnParams(n_inits=4,\n",
    "                                                             hidden_activation='softplus',\n",
    "                                                             output_activation='softmax',\n",
    "                                                             n_epochs=500,\n",
    "                                                             patience=50,\n",
    "                                                             batch_size=256,\n",
    "                                                             verbose=False)\n",
    "        trn_params.save(trn_params_folder)\n",
    "    else:\n",
    "        trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "        trn_params.load(trn_params_folder)\n",
    "\n",
    "    params_str = trn_params.get_params_str()\n",
    "    \n",
    "    # Choose how many points to be displayed in the plot\n",
    "    #neurons_mat = range(0,65,5)\n",
    "    \n",
    "    n_folds = len(CVO)\n",
    "    \n",
    "    # Initialize the matrices\n",
    "    acc_mat = np.zeros([n_folds,len(neurons_mat)])\n",
    "    sp_index_mat = np.zeros([n_folds,len(neurons_mat)])\n",
    "    efficiency_mat = np.zeros(shape=[n_folds, len(neurons_mat), len(class_labels)])\n",
    "    \n",
    "    for ifold in range(len(CVO)):\n",
    "        train_id, test_id = CVO[ifold]\n",
    "\n",
    "        # normalize data based in train set\n",
    "        if trn_params.params['norm'] == 'mapstd':\n",
    "            scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "            scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "        elif trn_params.params['norm'] == 'mapminmax':\n",
    "            scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "        norm_data = scaler.transform(all_data[test_id,:])\n",
    "\n",
    "        for ineuron,neuron_value in enumerate(neurons_mat):     \n",
    "            if neuron_value == 0:\n",
    "                neuron_value = 1\n",
    "            \n",
    "            model_str = '%s/%s/%s_%i_folds_%s_%i_neurons'%(save_path,analysis_str,\n",
    "                                                       model_prefix_str,\n",
    "                                                       n_folds,\n",
    "                                                       params_str,\n",
    "                                                       neuron_value)\n",
    "\n",
    "            if not development_flag:        \n",
    "                file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "            else:\n",
    "                file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "            if trn_params.params['verbose']:\n",
    "                print file_name\n",
    "                \n",
    "            # Check if the model has been trained    \n",
    "            if not os.path.exists(file_name):\n",
    "                def trainFold(ifold):\n",
    "                    ineuron = neuron_value\n",
    "                    return TrainFunctions.NeuralTrainFunction(data=all_data,\n",
    "                                            trgt=all_trgt,\n",
    "                                            ifold=ifold,\n",
    "                                            n_folds=n_folds, \n",
    "                                            n_neurons=ineuron,\n",
    "                                            trn_params=trn_params, \n",
    "                                            save_path=results_path,\n",
    "                                            dev=development_flag)\n",
    "            # Load weigths\n",
    "            model = load_model(file_name)\n",
    "            output = model.predict(norm_data)\n",
    "            # Get accuracy matrix\n",
    "            acc = sklearn.metrics.accuracy_score(trgt_sparse[test_id], np.round(output))\n",
    "            acc_mat[ifold,ineuron] = acc \n",
    "            # Get sp_index matrix\n",
    "            num_classes = len(class_labels)\n",
    "            efficiency = sklearn.metrics.recall_score(trgt_sparse[test_id], np.round(output), average=None)\n",
    "            sp_index = np.sqrt(np.sum(efficiency)/num_classes * np.power(np.prod(efficiency), 1.0/float(num_classes)))\n",
    "            sp_index_mat[ifold,ineuron] = sp_index\n",
    "            #Get efficiency per class matrix\n",
    "            efficiency = sklearn.metrics.recall_score(trgt_sparse[test_id], np.round(output), average=None)\n",
    "            efficiency_mat[ifold,ineuron] = efficiency\n",
    "            \n",
    "    joblib.dump([acc_mat,sp_index_mat,efficiency_mat,neurons_mat],analysis_file_name,compress=9)\n",
    "else:\n",
    "    [acc_mat,sp_index_mat,efficiency_mat,neurons_mat] = joblib.load(analysis_file_name) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "acc_mat = 100 * acc_mat\n",
    "sp_index_mat = 100 * sp_index_mat\n",
    "efficiency_mat = 100 * efficiency_mat\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8),nrows=1, ncols=1)\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "colors = ['b', 'r', 'g', 'y']\n",
    "markers = ['d-', '>--', '8-', 's--']\n",
    "\n",
    "# Plot Accuracy\n",
    "ax.errorbar(neurons_mat,np.mean(acc_mat,axis=0),\n",
    "            np.std(acc_mat,axis=0),fmt='o-.',\n",
    "            color='k',alpha=0.8,linewidth=2.5, label='Acc')\n",
    "# Plot SP index\n",
    "ax.errorbar(neurons_mat,np.mean(sp_index_mat,axis=0),\n",
    "            np.std(sp_index_mat,axis=0),fmt='^:',\n",
    "            color='k',alpha=0.8,linewidth=2.5, label='SP')\n",
    "# Plot effiency per class\n",
    "for iclass in range(len(class_labels)):\n",
    "    ax.errorbar(neurons_mat,np.mean(efficiency_mat[:,:,iclass],axis=0),\n",
    "                np.std(efficiency_mat[:,:,iclass],axis=0),fmt='-',\n",
    "                color=colors[iclass],alpha=0.3,linewidth=2.5, label='Eff. %s'%class_labels[iclass])\n",
    "\n",
    "ax.set_xlabel('Neurons Quantity',fontsize=18,weight='bold')\n",
    "ax.set_ylabel('Accuracy / SP Index / Eff. Per Class (%)',fontsize=18,weight='bold')\n",
    "ax.set_ylim([0, 105])\n",
    "ax.set_yticks(range(0,105,5))\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "# Set legend box location\n",
    "ax.legend(loc='upper right', ncol=6)\n",
    "\n",
    "# Subplot in the bigger plot\n",
    "# It is used to give emphasis in points of some range\n",
    "# These are in unitless percentages of the figure size. (0,0 is bottom left)\n",
    "rect = [0.36, 0.2, 0.6, 0.3] #x_location, y_location, width, height\n",
    "\n",
    "ax1 = FunctionsDataVisualization.add_subplot_axes(ax,rect)\n",
    "\n",
    "# Plot SP index\n",
    "ax1.errorbar(neurons_mat[70:105:5],np.mean(sp_index_mat[:,70:105:5],axis=0),\n",
    "            np.std(sp_index_mat[:,70:105:5],axis=0),fmt='^:',\n",
    "            color='k',alpha=0.8,linewidth=2.5)\n",
    "# Plot Accuracy \n",
    "ax1.errorbar(neurons_mat[70:105:5],np.mean(acc_mat[:,70:105:5],axis=0),\n",
    "            np.std(acc_mat[:,70:105:5],axis=0),fmt='o-',\n",
    "            color='k',alpha=0.8,linewidth=2.5)\n",
    "\n",
    "ax1.set_yticks(range(75,100,2))\n",
    "ax1.set_yticklabels(range(75,100,2), fontsize=14)\n",
    "\n",
    "ax1.set_xticks(range(70,105,5))\n",
    "ax1.set_xticklabels(range(70,105,5), fontsize=14)\n",
    "ax1.set_ylim([75,93])\n",
    "ax1.grid()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(pict_results_path+'/'+current_analysis+'_'+trn_params.get_params_str()+'.pdf')\n",
    "print \"Topology (%s)\"%trn_params.get_params_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "current_analysis = 'confusion_matrix'\n",
    "\n",
    "# Choose Topology\n",
    "ineuron = 100\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='relu',\n",
    "                                                         output_activation='softmax',\n",
    "                                                         n_epochs=300,\n",
    "                                                         patience=50,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "print \"Results for %i neurons (%s)\"%(ineuron, params_str)\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    plt.rcParams['xtick.labelsize'] = 15\n",
    "    plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "    plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "    plt.rc('font', weight='bold')\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data[test_id,:])\n",
    "\n",
    "    model_str = '%s/%s/%s_%i_folds_%s_%i_neurons'%(save_path,analysis_str,\n",
    "                                                   model_prefix_str,\n",
    "                                                   n_folds,\n",
    "                                                   params_str,\n",
    "                                                   ineuron)\n",
    "\n",
    "    if not development_flag:        \n",
    "        file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "    # Check if the topology has already been trained    \n",
    "    if not os.path.exists(file_name):\n",
    "        def trainFold(ifold):\n",
    "            return TrainFunctions.NeuralTrainFunction(data=all_data,\n",
    "                                    trgt=all_trgt,\n",
    "                                    ifold=ifold,\n",
    "                                    n_folds=n_folds, \n",
    "                                    n_neurons=ineuron,\n",
    "                                    trn_params=trn_params, \n",
    "                                    save_path=results_path,\n",
    "                                    dev=development_flag)\n",
    "    # load weights into new model\n",
    "    model = load_model(file_name)\n",
    "    output = np.round(model.predict(norm_data))\n",
    "    \n",
    "    all_output = np.argmax(output,axis=1)\n",
    "    cm = confusion_matrix(all_trgt[test_id], all_output)\n",
    "    cm_normalized = 100.*cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    im =ax.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Greys,clim=(0.0, 100.0))\n",
    "\n",
    "    width, height = cm_normalized.shape\n",
    "\n",
    "    for x in xrange(width):\n",
    "        for y in xrange(height):\n",
    "            if cm_normalized[x][y] < 50.:\n",
    "                ax.annotate('%1.3f%%'%(cm_normalized[x][y]), xy=(y, x),\n",
    "                            horizontalalignment='center',\n",
    "                            verticalalignment='center')\n",
    "            else:\n",
    "                ax.annotate('%1.3f%%'%(cm_normalized[x][y]), xy=(y, x),\n",
    "                            horizontalalignment='center',\n",
    "                            verticalalignment='center',color='white')\n",
    "\n",
    "    ax.set_title('Confusion Matrix',fontweight='bold',fontsize=15)\n",
    "    fig.colorbar(im)\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    ax.xaxis.set_ticks(tick_marks)\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "\n",
    "    ax.yaxis.set_ticks(tick_marks)\n",
    "    ax.yaxis.set_ticklabels(labels)\n",
    "\n",
    "    ax.set_ylabel('True Label',fontweight='bold',fontsize=15)\n",
    "    ax.set_xlabel('Predicted Label',fontweight='bold',fontsize=15)\n",
    "    \n",
    "    plt.savefig(pict_results_path+'/'+current_analysis+'_'+'%i_neurons_fold_%i_'%(ineuron,ifold)+trn_params.get_params_str()+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "%matplotlib inline  \n",
    "\n",
    "current_analysis = 'histogram'\n",
    "\n",
    "# Choose Topology\n",
    "ineuron = 100\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='relu',\n",
    "                                                         output_activation='softmax',\n",
    "                                                         n_epochs=300,\n",
    "                                                         patience=50,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "print \"Results for %i neurons (%s)\"%(ineuron, params_str)\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    plt.rcParams['xtick.labelsize'] = 15\n",
    "    plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "    plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "    plt.rc('font', weight='bold')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,20),nrows=4, ncols=4)\n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data[test_id,:])\n",
    "\n",
    "    model_str = '%s/%s/%s_%i_folds_%s_%i_neurons'%(save_path,analysis_str,\n",
    "                                                   model_prefix_str,\n",
    "                                                   n_folds,\n",
    "                                                   params_str,\n",
    "                                                   ineuron)\n",
    "\n",
    "    if not development_flag:        \n",
    "        file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "    # Check if the topology has already been trained    \n",
    "    if not os.path.exists(file_name):\n",
    "        NeuralTrainFunction(data=all_data,\n",
    "                            trgt=all_trgt,\n",
    "                            ifold=ifold,\n",
    "                            n_folds=n_folds, \n",
    "                            n_neurons=ineuron,\n",
    "                            trn_params=trn_params, \n",
    "                            save_path=results_path,\n",
    "                            dev=development_flag)  \n",
    "        \n",
    "    # load weights into new model\n",
    "    model = load_model(file_name)\n",
    "    output = np.round(model.predict(norm_data))\n",
    "   \n",
    "    from sklearn.neighbors import KernelDensity\n",
    "\n",
    "    m_bins = np.linspace(-0.5, 1.5, 1000)\n",
    "\n",
    "    m_colors = ['b', 'r', 'g', 'y']\n",
    "    \n",
    "    for i_target in range(trgt_sparse[test_id].shape[1]):\n",
    "        for i_output in range(output.shape[1]):\n",
    "            subplot_id = output.shape[1]*i_target+i_output\n",
    "            m_pts = output[all_trgt[test_id]==i_target,i_output]\n",
    "\n",
    "            n, bins, patches = ax[i_target,i_output].hist(m_pts,bins=m_bins,\n",
    "                                                          fc=m_colors[i_target],\n",
    "                                                          alpha=0.8, normed=1)\n",
    "\n",
    "            if i_output == 0:\n",
    "                ax[i_target,i_output].set_ylabel('Target %s'%(class_labels[i_target]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            if i_target == 3:\n",
    "                ax[i_target,i_output].set_xlabel('Output %s'%(class_labels[i_output]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            ax[i_target,i_output].grid()\n",
    "    plt.savefig(pict_results_path+'/'+current_analysis+'_'+'%i_neurons_fold_%i_'%(ineuron,ifold)+trn_params.get_params_str()+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kernel Density\n",
    "%matplotlib inline  \n",
    "\n",
    "current_analysis = 'kernel_density'\n",
    "\n",
    "# Choose Topology\n",
    "ineuron = 30\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='relu',\n",
    "                                                         output_activation='softmax',\n",
    "                                                         n_epochs=300,\n",
    "                                                         patience=50,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "print \"Results for %i neurons (%s)\"%(ineuron, params_str)\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    plt.rcParams['xtick.labelsize'] = 15\n",
    "    plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "    plt.rc('legend',**{'fontsize':15})\n",
    "\n",
    "    plt.rc('font', weight='bold')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,20),nrows=4, ncols=4)\n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data[test_id,:])\n",
    "\n",
    "    model_str = '%s/%s/%s_%i_folds_%s_%i_neurons'%(save_path,analysis_str,\n",
    "                                                   model_prefix_str,\n",
    "                                                   n_folds,\n",
    "                                                   params_str,\n",
    "                                                   ineuron)\n",
    "\n",
    "    if not development_flag:        \n",
    "        file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "    # Check if the topology has already been trained    \n",
    "    if not os.path.exists(file_name):\n",
    "        NeuralTrainFunction(data=all_data,\n",
    "                            trgt=all_trgt,\n",
    "                            ifold=ifold,\n",
    "                            n_folds=n_folds, \n",
    "                            n_neurons=ineuron,\n",
    "                            trn_params=trn_params, \n",
    "                            save_path=results_path,\n",
    "                            dev=development_flag)  \n",
    "        \n",
    "    # load weights into new model\n",
    "    model = load_model(file_name)\n",
    "    output = np.round(model.predict(norm_data))\n",
    "   \n",
    "    from sklearn.neighbors import KernelDensity\n",
    "\n",
    "    m_bins = np.linspace(-0.5, 1.5,100)\n",
    "\n",
    "    m_colors = ['b', 'r', 'g', 'y']\n",
    "\n",
    "    kernel = 'gaussian' # other kernels: 'gaussian', 'tophat', \n",
    "                        #'epanechnikov', 'exponential', 'linear', 'cosine'\n",
    "    for i_target in range(trgt_sparse[test_id].shape[1]):\n",
    "        for i_output in range(output.shape[1]):\n",
    "            subplot_id = output.shape[1]*i_target+i_output\n",
    "            m_pts = output[all_trgt[test_id]==i_target,i_output]\n",
    "\n",
    "            kde = KernelDensity(kernel=kernel,algorithm='auto',\n",
    "                                bandwidth=0.5).fit(m_pts[:, np.newaxis])\n",
    "            log_dens_x = kde.score_samples(m_bins[:, np.newaxis])\n",
    "            ax[i_target,i_output].plot(m_bins, np.exp(log_dens_x),\n",
    "                                       color=m_colors[i_target],linewidth=2.0)\n",
    "            if i_output == 0:\n",
    "                ax[i_target,i_output].set_ylabel('Target %s'%(class_labels[i_target]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            if i_target == 3:\n",
    "                ax[i_target,i_output].set_xlabel('Output %s'%(class_labels[i_output]),\n",
    "                                                 fontweight='bold',fontsize=15)\n",
    "            ax[i_target,i_output].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "# Choose Topology\n",
    "ineuron = 100\n",
    "\n",
    "acc = np.zeros([4, 4]);\n",
    "\n",
    "# Parameters\n",
    "trn_params_folder='%s/%s/%s_trnparams.jbl'%(results_path,analysis_str,analysis_name)\n",
    "if not os.path.exists(trn_params_folder):\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='relu',\n",
    "                                                         output_activation='softmax',\n",
    "                                                         n_epochs=300,\n",
    "                                                         patience=50,\n",
    "                                                         batch_size=256,\n",
    "                                                         verbose=False)\n",
    "    trn_params.save(trn_params_folder)\n",
    "else:\n",
    "    trn_params = trnparams.NeuralClassificationTrnParams()\n",
    "    trn_params.load(trn_params_folder)\n",
    "\n",
    "params_str = trn_params.get_params_str()\n",
    "print \"Results for %i neurons (%s)\"%(ineuron, params_str)\n",
    "for ifold in range(len(CVO)):\n",
    "    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    labels = class_labels.values()\n",
    "\n",
    "    # normalize data based in train set\n",
    "    if trn_params.params['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "    elif trn_params.params['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "    norm_data = scaler.transform(all_data)\n",
    "    norm_data = norm_data[test_id,:]\n",
    "    model_str = '%s/%s/%s_%i_folds_%s_%i_neurons'%(save_path,analysis_str,\n",
    "                                                   model_prefix_str,\n",
    "                                                   n_folds,\n",
    "                                                   params_str,\n",
    "                                                   ineuron)\n",
    "\n",
    "    if not development_flag:        \n",
    "        file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "    else:\n",
    "        file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "    # Check if the topology has already been trained    \n",
    "    if not os.path.exists(file_name):\n",
    "        NeuralTrainFunction(data=all_data,\n",
    "                            trgt=all_trgt,\n",
    "                            ifold=ifold,\n",
    "                            n_folds=n_folds, \n",
    "                            n_neurons=ineuron,\n",
    "                            trn_params=trn_params, \n",
    "                            save_path=results_path,\n",
    "                            dev=development_flag)  \n",
    "        \n",
    "    # load weights into new model\n",
    "    model = load_model(file_name)\n",
    "    # Winner takes all\n",
    "    output = np.round(model.predict(norm_data))\n",
    "    \n",
    "    num_classes = len(class_labels.keys())\n",
    "    \n",
    "    efficiency = sklearn.metrics.recall_score(trgt_sparse[test_id], np.round(output), average=None)\n",
    "    sp_index = np.sum(efficiency)/num_classes * np.power(np.prod(efficiency), 1/num_classes)\n",
    "    sp_index = np.sqrt(sp_index)\n",
    "    acc[ifold] = efficiency\n",
    "    precision = sklearn.metrics.precision_score(trgt_sparse[test_id], np.round(output), average=None)\n",
    "    f1_score = sklearn.metrics.f1_score(trgt_sparse[test_id], np.round(output), average=None)\n",
    "    \n",
    "    print '\\tPrecision\\tEfficiency\\tF1_Score'\n",
    "    for iclass in range(num_classes):\n",
    "        print '%s:\\t%f\\t%f\\t%f\\n'%(class_labels[iclass], precision[iclass], efficiency[iclass],f1_score[iclass])\n",
    "    print 'SP index: %f\\n'%sp_index\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Paramétrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# analysis example - number epochs\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "from Functions import TrainParameters as trnparams\n",
    "\n",
    "# generate analysis data\n",
    "save_path=results_path\n",
    "\n",
    "analysis_str = 'NeuralNetwork'\n",
    "model_prefix_str = 'RawData'\n",
    "parameter = 'epochs'\n",
    "\n",
    "analysis_file_name='%s/%s/%s_%s_sweep.jbl'%(results_path,analysis_str,analysis_name, parameter)\n",
    "\n",
    "if not os.path.exists(analysis_file_name):\n",
    "    \n",
    "    epochs_mat = range(1,10,1)\n",
    "    mse_mat = np.zeros([n_folds,len(epochs_mat)])\n",
    "    \n",
    "    for iepochs,epochs_value in enumerate(epochs_mat):       \n",
    "        print \"Number of Epochs:\", epochs_value\n",
    "        trn_params = trnparams.NeuralClassificationTrnParams(n_inits=1,\n",
    "                                                         hidden_activation='softplus',\n",
    "                                                         output_activation='softmax',\n",
    "                                                         patience=50,\n",
    "                                                         batch_size=512,\n",
    "                                                         n_epochs=epochs_value)\n",
    "    \n",
    "        params_str = trn_params.get_params_str()\n",
    "        print \"params_str: \",params_str\n",
    "        n_folds = 2\n",
    "        CVO = trnparams.ClassificationFolds(folder=results_path,n_folds=n_folds,trgt=all_trgt,dev=development_flag)\n",
    "\n",
    "\n",
    "        for ifold in range(len(CVO)):\n",
    "            train_id, test_id = CVO[ifold]\n",
    "\n",
    "            # normalize data based in train set\n",
    "            if trn_params.params['norm'] == 'mapstd':\n",
    "                scaler = preprocessing.StandardScaler().fit(all_data[train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapstd_rob':\n",
    "                scaler = preprocessing.RobustScaler().fit(all_data[train_id,:])\n",
    "            elif trn_params.params['norm'] == 'mapminmax':\n",
    "                scaler = preprocessing.MinMaxScaler().fit(all_data[train_id,:])\n",
    "\n",
    "            norm_data = scaler.transform(all_data)\n",
    "            \n",
    "            neurons =10\n",
    "            NeuralTrainFunction(data=all_data, trgt=all_trgt, ifold=ifold, n_folds=n_folds, \n",
    "                                n_neurons=neurons, trn_params=trn_params, \n",
    "                                save_path=results_path,dev=development_flag)\n",
    "\n",
    "            # turn targets in sparse mode\n",
    "            trgt_sparse = np_utils.to_categorical(all_trgt.astype(int))\n",
    "\n",
    "\n",
    "            model_str = '%s/%s/%s_%i_folds_%s_%i_neurons'%(save_path,analysis_str,\n",
    "                                                           model_prefix_str,\n",
    "                                                           n_folds,\n",
    "                                                           params_str,\n",
    "                                                           neurons)\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Dense(neurons, input_dim=data.shape[1], init=\"uniform\"))\n",
    "            model.add(Activation(trn_params.params['hidden_activation']))\n",
    "            model.add(Dense(trgt_sparse.shape[1], init=\"uniform\")) \n",
    "            model.add(Activation(trn_params.params['output_activation']))\n",
    "\n",
    "            if not development_flag:        \n",
    "                file_name = '%s_fold_%i_model.h5'%(model_str,ifold)\n",
    "            else:\n",
    "                file_name = '%s_fold_%i_model_dev.h5'%(model_str,ifold)\n",
    "                \n",
    "            print 'Model name: %s'%(file_name)\n",
    "            model = load_model(file_name)\n",
    "            output = model.predict(norm_data)\n",
    "            mse = np.mean(np.sum((trgt_sparse-output)**2))\n",
    "            mse_mat[ifold,iepochs] = mse\n",
    "    joblib.dump([mse_mat,epochs_mat],analysis_file_name,compress=9)\n",
    "else:\n",
    "    [mse_mat,epochs_mat] = joblib.load(analysis_file_name) \n",
    "\n",
    "# plot analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10),nrows=1, ncols=1)\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rc('legend',**{'fontsize':15})\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "ax.errorbar(epochs_mat,np.mean(mse_mat,axis=0),\n",
    "            np.std(mse_mat,axis=0),fmt='o-',\n",
    "            color='k',alpha=0.7,linewidth=2.5)\n",
    "ax.set_title('MSE per Number of Epochs',fontsize=18,weight='bold')\n",
    "ax.set_xlabel('Epochs',fontsize=18,weight='bold')\n",
    "ax.set_ylabel('MSE Value',fontsize=18,weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
