{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Projeto de Classificaçãoo para Marinha do Brasil\n",
    "### Autor: Natanael Junior (natmourajr@gmail.com)\n",
    "Laboratório de Processamento de Sinais - UFRJ\n",
    "\n",
    "Laboratório de Tecnologia Sonar\n",
    "\n",
    "Instituto de Pesquisas da Marinha - IPqM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas e leitura de dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to import all libraries: 2.6941299438476562e-05 seconds\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'SONAR_WORKSPACE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-afa40d9351d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moutputpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OUTPUTDATAPATH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmain_analysis_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SONAR_WORKSPACE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mlog_analysis_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PACKAGE_OUTPUT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mresult_analysis_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PACKAGE_OUTPUT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/StackedAutoEncoder'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sonar/lib/python3.6/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SONAR_WORKSPACE'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "m_time = time.time()\n",
    "print('Time to import all libraries: '+str(m_time-init_time)+' seconds')\n",
    "\n",
    "outputpath = os.environ['OUTPUTDATAPATH']\n",
    "main_analysis_path = os.environ['SONAR_WORKSPACE']\n",
    "log_analysis_path = os.environ['PACKAGE_OUTPUT']\n",
    "result_analysis_path = os.environ['PACKAGE_OUTPUT']+'/StackedAutoEncoder'\n",
    "pict_results_path = result_analysis_path+'/picts'\n",
    "files_results_path = result_analysis_path+'/output_files'\n",
    "\n",
    "# Read data\n",
    "# Check if LofarData has created...\n",
    "m_time = time.time()\n",
    "\n",
    "\n",
    "subfolder = '4classes_old'\n",
    "n_pts_fft = 1024\n",
    "decimation_rate = 3\n",
    "\n",
    "if(not os.path.exists(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))):\n",
    "    print(outputpath+'/'+'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "        subfolder,n_pts_fft,decimation_rate)+' doesnt exist...please create it')\n",
    "    exit()\n",
    "    \n",
    "#Read lofar data\n",
    "[data,class_labels] = joblib.load(outputpath+'/'+\n",
    "                                  'LofarData_%s_%i_fft_pts_%i_decimation_rate.jbl'%(\n",
    "            subfolder,n_pts_fft,decimation_rate))\n",
    "m_time = time.time()-m_time\n",
    "print('Time to read data file: '+str(m_time)+' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definições do treinamento\n",
    "\n",
    "Nessa célula temos os parâmetros do treinamento a ser realizado. No log, deve ficar armazenada a data do treinamento para a reconstrução dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "Dividing data in trn and tst\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/StackedAutoEncoder/train_info_files/2017_03_08_12_27_21_train_info.jbl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from Functions import LogFunctions as log\n",
    "\n",
    "# Create a entry in log file\n",
    "m_log = log.LogInformation()\n",
    "date = m_log.CreateLogEntry(\"Classification\",'ConvNeuralNetwork')\n",
    "\n",
    "# Create a train information file\n",
    "n_folds = 2\n",
    "n_inits = 1\n",
    "norm = 'mapstd'\n",
    "qtd_window_in_event = 10\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_inits'] = n_inits\n",
    "train_info['norm'] = norm\n",
    "train_info['qtd_window_in_event'] = qtd_window_in_event\n",
    "\n",
    "# divide data in train and test for novelty detection\n",
    "print 'Dividing data in trn and tst'\n",
    "CVO = cross_validation.StratifiedKFold(all_trgt, n_folds)\n",
    "CVO = list(CVO)\n",
    "train_info['CVO'] = CVO\n",
    "\n",
    "train_info['topologic_anal_done'] = False\n",
    "train_info['train_done'] = False\n",
    "train_info['results_done'] = False\n",
    "\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+date+'_train_info.jbl'\n",
    "joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'date': '2017_03_07_23_34_47', 'package': 'StackedAutoEncoder'}, 1: {'date': '2017_03_07_23_35_21', 'package': 'StackedAutoEncoder'}, 2: {'date': '2017_03_08_00_07_14', 'package': 'StackedAutoEncoder'}, 3: {'date': '2017_03_08_00_20_36', 'package': 'StackedAutoEncoder'}, 4: {'date': '2017_03_08_11_52_41', 'package': 'StackedAutoEncoder'}, 5: {'date': '2017_03_08_11_58_10', 'package': 'StackedAutoEncoder'}, 6: {'date': '2017_03_08_12_03_24', 'package': 'ConvNeuralNetwork'}, 7: {'date': '2017_03_08_12_05_27', 'package': 'StackedAutoEncoder'}, 8: {'date': '2017_03_08_12_27_09', 'package': 'ConvNeuralNetwork'}, 9: {'date': '2017_03_08_12_27_21', 'package': 'ConvNeuralNetwork'}}\n"
     ]
    }
   ],
   "source": [
    "# Read log files\n",
    "from Functions import LogFunctions as log\n",
    "mlog = log.LogInformation()\n",
    "log_entries = mlog.RecoverLogEntries(package_name=\"Classification\")\n",
    "print log_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing train performed in 2017_03_08_12_27_21 and for ConvNeuralNetwork analysis\n",
      "StackedAutoEncoder Train Info File\n",
      "Date: 2017_03_08_12_27_21\n",
      "Number of Folds: 2\n",
      "Number of Inits: 1\n",
      "Line quantity in each event: 10\n",
      "Topological Analysis Done: False\n",
      "Train Done: False\n",
      "Extract Results: False\n"
     ]
    }
   ],
   "source": [
    "# Read Information of Train Info File\n",
    "choose_date = '2017_03_08_12_27_21'\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'ConvNeuralNetwork':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'Analysing train performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    print 'StackedAutoEncoder Train Info File'\n",
    "    print 'Date: %s'%(choose_date)\n",
    "    print 'Number of Folds: %i'%(train_info['n_folds'])\n",
    "    print 'Number of Inits: %i'%(train_info['n_inits'])\n",
    "    print 'Line quantity in each event: %i'%(train_info['qtd_window_in_event'])\n",
    "    \n",
    "    if train_info['topologic_anal_done']:\n",
    "        print 'Topological Analysis Done: True'\n",
    "    else:\n",
    "        print 'Topological Analysis Done: False'\n",
    "    if train_info['train_done']:\n",
    "        print 'Train Done: True'\n",
    "    else:\n",
    "        print 'Train Done: False'\n",
    "    if train_info['results_done']:\n",
    "        print 'Extract Results: True'\n",
    "    else:\n",
    "        print 'Extract Results: False'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento dos dados\n",
    "\n",
    "Os dados encontram-se no formato do matlab, para isso precisam ser processados para o formato de python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process data...\n",
    "# create a full data vector\n",
    "all_data = {};\n",
    "all_trgt = {};\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    for irun in range(len(data[iclass])):\n",
    "        if len(all_data) == 0:\n",
    "            all_data = data[iclass][irun]['Signal']\n",
    "            all_trgt = (iclass)*np.ones(data[iclass][irun]['Signal'].shape[1])\n",
    "        else:\n",
    "            all_data = np.append(all_data,data[iclass][irun]['Signal'],axis=1)\n",
    "            all_trgt = np.append(all_trgt,(iclass)*np.ones(data[iclass][irun]\n",
    "                                                           ['Signal'].shape[1]),axis=0)\n",
    "            \n",
    "all_data = all_data.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceamento de Classes\n",
    "Os dados encontram-se desbalanceados. Com isso, os classificadores podem se especializar em uma classe (gerando mais SVs para a mesma) e não se especializar em outras\n",
    "\n",
    "Acessados em 21/12/2016\n",
    "\n",
    "https://svds.com/learning-imbalanced-classes/\n",
    "\n",
    "http://www.cs.utah.edu/~piyush/teaching/ImbalancedLearning.pdf\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html\n",
    "\n",
    "Para solucionar isso, a primeira solução é \"criar\" dados das classes com menos eventos de maneira aleatória. Outras soluções podem ser propostas posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd event of ClassA is 9781\n",
      "Qtd event of ClassB is 9781\n",
      "Qtd event of ClassC is 9781\n",
      "Qtd event of ClassD is 9781\n",
      "\n",
      "Biggest class is ClassA with 9781 events\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (9781, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (9781, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (9781, 400)\n",
      "DataHandler Class: CreateEventsForClass\n",
      "Original Size: (9781, 400)\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "# unbalanced data to balanced data with random data creation of small classes\n",
    "\n",
    "# Same number of events in each class\n",
    "qtd_events_biggest_class = 0\n",
    "biggest_class_label = ''\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if sum(all_trgt==iclass) > qtd_events_biggest_class:\n",
    "        qtd_events_biggest_class = sum(all_trgt==iclass)\n",
    "        biggest_class_label = class_label\n",
    "    print \"Qtd event of %s is %i\"%(class_label,sum(all_trgt==iclass))\n",
    "print \"\\nBiggest class is %s with %i events\"%(biggest_class_label,qtd_events_biggest_class)\n",
    "\n",
    "\n",
    "balanced_data = {}\n",
    "balanced_trgt = {}\n",
    "\n",
    "from Functions import DataHandler as dh\n",
    "m_datahandler = dh.DataHandlerFunctions()\n",
    "\n",
    "for iclass, class_label in enumerate(class_labels):\n",
    "    if len(balanced_data) == 0:\n",
    "        class_events = all_data[all_trgt==iclass,:]\n",
    "        balanced_data = m_datahandler.CreateEventsForClass(\n",
    "            class_events,qtd_events_biggest_class-(len(class_events)))\n",
    "        balanced_trgt = (iclass)*np.ones(qtd_events_biggest_class)\n",
    "    else:\n",
    "        balanced_data = np.append(balanced_data,\n",
    "                                  (m_datahandler.CreateEventsForClass(\n",
    "                    all_data[all_trgt==iclass,:],\n",
    "                    qtd_events_biggest_class-sum(all_trgt==iclass))),\n",
    "                                  axis=0)\n",
    "        balanced_trgt = np.append(balanced_trgt,\n",
    "                                  (iclass)*np.ones(qtd_events_biggest_class),axis=0)\n",
    "        \n",
    "all_data = balanced_data\n",
    "all_trgt = balanced_trgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform a line of lofar in an \"image\" (sequence of lines)\n",
    "new_data = np.zeros([all_data.shape[0],\n",
    "                     all_data.shape[1],\n",
    "                     train_info['qtd_window_in_event']])\n",
    "\n",
    "class_count = 0\n",
    "for ievent in range(all_data.shape[0]):\n",
    "        if ievent > all_data.shape[0]-train_info['qtd_window_in_event']:\n",
    "            continue\n",
    "            \n",
    "        if class_count < qtd_events_biggest_class-train_info['qtd_window_in_event']:\n",
    "            new_data[ievent,:,:] = (all_data[ievent:ievent+\n",
    "                                             train_info['qtd_window_in_event'],:].T)\n",
    "            class_count = class_count+1\n",
    "        else:\n",
    "            if all_trgt[ievent-1] != all_trgt[ievent]:\n",
    "                class_count = 0\n",
    "                \n",
    "all_window_data = new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First approach: Vary number of filter and their size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n",
      "Analysis performed in 2017_03_08_12_27_21 and for ConvNeuralNetwork analysis\n",
      "Train on 19560 samples, validate on 19564 samples\n",
      "Epoch 1/10\n",
      " 3904/19560 [====>.........................] - ETA: 519s - loss: 0.1295 - mean_squared_error: 0.0168 - acc: 0.9541"
     ]
    }
   ],
   "source": [
    "# CNN parameters extraction\n",
    "\n",
    "%time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import keras.callbacks as callbacks\n",
    "\n",
    "from Functions import PreProcessing as preproc\n",
    "\n",
    "\n",
    "models = {}\n",
    "trn_descs = {}\n",
    "\n",
    "\n",
    "# turn targets in sparse mode\n",
    "n_classes = 4\n",
    "trgt_sparse = np_utils.to_categorical(all_trgt,n_classes)\n",
    "\n",
    "for log_id, log_entry in enumerate(log_entries):\n",
    "    if log_entries[log_id]['package'] != 'ConvNeuralNetwork':\n",
    "        continue\n",
    "    if log_entries[log_id]['date'] != choose_date:\n",
    "        continue\n",
    "    print 'Analysis performed in %s and for %s analysis'%(\n",
    "        log_entries[log_id]['date'],log_entries[log_id]['package'])\n",
    "    \n",
    "    # Read train info file\n",
    "    train_info_name = '%s/train_info_files/%s_train_info.jbl'%(\n",
    "        result_analysis_path,log_entries[log_id]['date'])\n",
    "    \n",
    "    [train_info] = joblib.load(train_info_name)\n",
    "    \n",
    "    # saving time\n",
    "    if train_info['topologic_anal_done']:\n",
    "        print 'Topological Analysis is done, just analyse it'\n",
    "        continue\n",
    "    \n",
    "    trn_params = preproc.TrnParams(learning_rate=0.5,verbose=True,\n",
    "                                   train_verbose=True, n_epochs=10)\n",
    "    \n",
    "    for ifold in range(train_info['n_folds']):\n",
    "        train_id, test_id = train_info['CVO'][ifold]\n",
    "        \n",
    "        # normalização\n",
    "        # como normalizar essa droga!?!?!?! Por enquanto, não normalizo!!!\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Convolution1D(60, 10, border_mode='same',\n",
    "                                input_shape=all_window_data.shape[1:],))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution1D(60, 10))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Convolution1D(30, 10))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(n_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        # Let's train the model using RMSprop\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['mean_squared_error','accuracy'])\n",
    "        \n",
    "        # Early Stopping\n",
    "        earlyStopping = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                patience=25, \n",
    "                                                verbose=trn_params.train_verbose, \n",
    "                                                mode='auto')\n",
    "                \n",
    "        \n",
    "        trn_desc = model.fit(all_window_data[train_id,:,:], trgt_sparse[train_id],\n",
    "                             batch_size=trn_params.batch_size,\n",
    "                             nb_epoch=trn_params.n_epochs,\n",
    "                             verbose=trn_params.verbose,\n",
    "                             callbacks=[earlyStopping],\n",
    "                             validation_data=(all_window_data[test_id,:,:], \n",
    "                                              trgt_sparse[test_id]),\n",
    "                             shuffle=True)\n",
    "       \n",
    "        models[ifold] = model\n",
    "        trn_descs[ifold] = trn_desc\n",
    "    \n",
    "    print 'Train done'\n",
    "    # saving file\n",
    "    \n",
    "    for ifold in range(train_info['n_folds']):\n",
    "        model_name = (result_analysis_path+'/output_files'+'/'+\n",
    "                      choose_date+'_model_fold'+str(ifold)+'.h5')\n",
    "        models[ifold].save(model_name)\n",
    "        \n",
    "    train_info['topologic_anal_done'] = True\n",
    "    joblib.dump([train_info],train_info_name,compress=9)\n",
    "    \n",
    "    # save trn_desc\n",
    "    trn_desc_file = open(result_analysis_path+'/output_files'+'/'+'trn_desc_'+choose_date+'.pickle', \"wb\")\n",
    "    pickle.dump([trn_descs],trn_desc_file)\n",
    "    trn_desc_file.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/natmourajr/Workspace/Doutorado/SonarAnalysis/Results/Classification/StackedAutoEncoder/train_info_files/2017_03_08_12_27_21_train_info.jbl']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to reset analysis\n",
    "train_info_name = result_analysis_path+'/train_info_files'+'/'+choose_date+'_train_info.jbl'\n",
    "\n",
    "train_info['topologic_anal_done'] = False\n",
    "joblib.dump([train_info],train_info_name,compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_desc_file = open(result_analysis_path+'/output_files'+'/'+'trn_desc_'+choose_date+'.pickle', \"r\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['legend.numpoints'] = 1\n",
    "plt.rcParams['legend.handlelength'] = 3\n",
    "plt.rcParams['legend.borderpad'] = 0.3\n",
    "    \n",
    "fig1 = plt.figure(figsize=(10,6))\n",
    "\n",
    "\n",
    "ifold = 0\n",
    "\n",
    "l1 = plt.plot(trn_descs[ifold].epoch,\n",
    "              trn_descs[ifold].history['loss'],color=[0,0,1],\n",
    "              linewidth=2.5,linestyle='solid',label='Train Perf.')\n",
    "l2 = plt.plot(trn_descs[ifold].epoch,\n",
    "              trn_descs[ifold].history['val_loss'],color=[1,0,0],\n",
    "              linewidth=2.5,linestyle='dashed',label='Test Perf.')\n",
    "cost = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
